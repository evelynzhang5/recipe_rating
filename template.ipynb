{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe Rating Predictions Based on MEAT Inclusion\n",
    "\n",
    "**Name(s)**: Evelyn Zhang, Haowen Zhang\n",
    "\n",
    "**Website Link**: https://evelynzhang5.github.io/recipe_rating/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.652554Z",
     "start_time": "2019-10-31T23:36:27.180520Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import plotly.express as px\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "from dsc80_utils import * \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer,RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the past few years there has been a major change in dietary preferences and more people are paying attention to how the composition of recipes influences consumer perceptions and ratings. Specifically, the inclusion of meat in recipes has become a focal point, as trends toward plant-based eating has been popularized with our increasing health-conscious and environmental awareness.\n",
    "\n",
    "Therefore, we are interested in investigating what kind of reciptes tend to have a higher ratings and hence predict the ratings of reciptes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the following steps to clean our data:\n",
    "\n",
    "1. Left merge `recipe` and `interactions` dataset. \n",
    "\n",
    "2. Check the data type for resulting columns\n",
    "\n",
    "3.  Fill all ratings of **0** in `rating` with **np.nan** and convert th rest to integers.\n",
    "    - All the ratings have scale from **1** to **5** with **1** meaning the lowest rating **5** means the highest rating; therefore, a rating of **0** suggests that the rating for that speicifc recipe is missing. Thus, to avoid bias in the `ratings` and false results during our permutation test, we filled the value **0** with **np.nan**. Since we plan to build classifier, we changed the type to interger.\n",
    "\n",
    "4. Check the **null** values across columns. Drop one row with `name` of the recipe being **nan**.\n",
    "\n",
    "5. Add column `average_rating` containing average ratings per recipe.\n",
    "\n",
    "    - Since a recipe can have numerous ratings from different users while we are merging the data, we take the **mean** of all the ratings, which will give us a more comprehensive understanding of all the ratings for the given recipe.\n",
    "\n",
    "6. Split and convert the `nutritions`, `ingredients`, `tags` column from `str` to `list`.\n",
    "\n",
    "    - Since we will be doing multiple extractions and look up of items in each column for future analysis, we thought of converting these columns which are **objects** types, acting like strings, to **list** type. Speicifically, we applied a lambda function to perform simple strip() and split() then converted the columns to **list** of strings or floats depending on the content. It will allow us to conduct numerical calculations, item lookup, and so forth on the columns.\n",
    "\n",
    "7. Split values in the `nutrition` column to individual columns of floats\n",
    "\n",
    "    - After converting the column to **list**, we then seperate each into a column with thecorresponding nutrition information as **float**. This could help us look up for speicifc nutrition information more quickly.\n",
    "\n",
    "8. Convert `submitted` and `date` to datetime.\n",
    "\n",
    "    - Sinnce these two columns are both stored as **objects**, for our convenience, we decidded to convert them into datetime to allow us conduct analysis on selected period of time.\n",
    "\n",
    "9. Add `contains_meat` to the dataframe\n",
    "\n",
    "    - The `contains_meat`is a boolean column checking if the tags of recipes contain `meat`. This step separates the recipes into two groups, with recipes contain `meat` and those without. This provides us a convenient way to compare ratings of recipes of two distributitons: recipes with and without `meat`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes= pd.read_csv('RAW_recipes.csv')\n",
    "interactions = pd.read_csv('RAW_interactions.csv')\n",
    "recipes_df = recipes.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "# Left merge the recipes and interactions datasets on 'id' (recipe_id in interactions_df)\n",
    "merged_df = recipes_df.merge(interactions, left_on='id', right_on='recipe_id', how='left')\n",
    "# Convert rating column to numeric explicitly\n",
    "merged_df['rating'] = pd.to_numeric(merged_df['rating'], errors='coerce')\n",
    "# Replace all ratings of 0 with NaN\n",
    "merged_df.loc[merged_df['rating'] == 0, 'rating'] = np.nan\n",
    "# Calculate the average rating per recipe\n",
    "average_ratings = merged_df.groupby('id')['rating'].mean()\n",
    "# Merge the average ratings back into the recipes dataset\n",
    "recipes_df = merged_df.merge(average_ratings, on='id', how='left').rename(columns = {'rating_y':'average_rating'}).rename(columns = {'rating_x':'rating'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = recipes_df\n",
    "cleaned_df['nutrition'] = cleaned_df['nutrition'].apply(lambda x: (x.strip(\"[]\").split(\",\")))\n",
    "cleaned_df['ingredients'] = cleaned_df['ingredients'].apply(lambda x: (x.strip(\"[]\").split(\",\")))\n",
    "cleaned_df['date']= pd.to_datetime(cleaned_df['date'])\n",
    "cleaned_df['submitted']=pd.to_datetime(cleaned_df['submitted'])\n",
    "cleaned_df['tags'] = cleaned_df['tags'].apply(lambda x: [item.strip(\" '\") for item in x.strip('[]').split(',') if item.strip(\" '\")])\n",
    "cleaned_df['contains_meat']=cleaned_df['tags'].apply(lambda x: 'meat' in x)\n",
    "# Expand the list into individual columns. Adjust the column names as desired.\n",
    "cleaned_df[['calories', 'total_fat', 'sugar', 'sodium', 'protein', 'saturated_fat', 'carbohydrates']] = \\\n",
    "    pd.DataFrame(cleaned_df['nutrition'].tolist(), index=cleaned_df.index)\n",
    "# Convert the new columns to float type\n",
    "cols = ['calories', 'total_fat', 'sugar', 'sodium', 'protein', 'saturated_fat', 'carbohydrates']\n",
    "cleaned_df[cols] = cleaned_df[cols].astype(float)\n",
    "cleaned_df['rating']= cleaned_df['rating'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "cleaned_df= cleaned_df.set_index('name')\n",
    "cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univarate Analysis\n",
    "Since we would like to classify the ratings in the end, we would like to understand the distribution of our `average_ratings`. As we can observe in the graph, the recipe ratings are more concentrated towards 4 or 5, meaning there is a clear left skew. We suspect that the `average_ratings` might be biased and people tend to give ratings for review especially when they particular like the receipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "#pio.renderers.default = \"iframe\"\n",
    "pio.renderers.default = \"notebook\"\n",
    "# or\n",
    "fig = px.histogram(cleaned_df, x='average_rating', nbins=5, title='Distribution of average rating across all unique recipes')\n",
    "fig.update_traces(xbins=dict(start=1, end=5, size=1))\n",
    "fig.show()\n",
    "fig.write_html('rating-distribution.html', include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to analyze the relationships between the ratings and whether the recipes contains meat. The protein content is associated with whether the recipe contains meat. Se we make a histogram to learn the distribution of `protein`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "#pio.renderers.default = \"iframe\"\n",
    "pio.renderers.default = \"notebook\"\n",
    "# or\n",
    "\n",
    "fig = px.histogram(cleaned_df, x='protein', nbins=5, title='Distribution of protein(PDV) across recipes')\n",
    "fig.update_traces(xbins=dict(start=0, end=220, size=10))\n",
    "fig.show()\n",
    "fig.write_html('protein-distribution.html', include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we learn that the distribution of protein skews to the right. Most of the dishes have protein content between 0 and 25. This might indicate that recipe containing meat may lead to lower ratings, we will test it later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Analysis\n",
    "\n",
    "To investigate our most interested question,**will the meat inclusion(with meat tag) affect recipes' ratings**, we used a KDE plot to show the distribution of average ratings for recipes with and without the meat tag. We can see that\n",
    "both curves, with and without meat tags, have a major peak around a certain rating (around 4-5). However, in general, the close alignment of the two KDE curves implies that the presence or absence of meat **does not** significantly shift the overall rating distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mpld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a figure and axis object\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create the KDE plot with a separate density for recipes with and without meat\n",
    "sns.kdeplot(\n",
    "    data=cleaned_df.reset_index(),\n",
    "    x=\"rating\",\n",
    "    hue=\"contains_meat\",\n",
    "    fill=True,           # Fills the area under each KDE curve\n",
    "    common_norm=False,   # Normalize densities independently\n",
    "    ax=ax\n",
    ")\n",
    "# Set title and labels\n",
    "ax.set_title(\"KDE Plot of Meat Tag Distribution by Rating\")\n",
    "ax.set_xlabel(\"Rating\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "import mpld3\n",
    "mpld3.save_html(fig, \"KDE.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, in this KDE graph between **submitted_year** and **rating**, we see that more ratings and higher ratings are given in earlier years. In other words, recipes submitted in recent years has a lower rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df = cleaned_df.copy().dropna()\n",
    "df['submitted_year'] = df['submitted'].dt.year\n",
    "\n",
    "sns.kdeplot(data=df, x=\"submitted_year\", hue=\"rating\", fill=True)\n",
    "plt.xlabel(\"Submission Year\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"KDE of Submission Year by Rating\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting Aggregates\n",
    "For our aggregating analysis, we would like to use pivot table to compare and contrast the difference in `rating`, `protein`, `minutes` and `total_fat ` between two distributions in `contains_meat` group (with and without meat tag)\n",
    " Interestingly, we can see that recipes with meat deliver higher protein but also come with higher fat content and longer preparation times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a pivot table that computes the mean values for average_rating and total_fat\n",
    "pivot_table = cleaned_df.pivot_table(\n",
    "    index='contains_meat',\n",
    "    values=['rating', 'total_fat','protein','minutes'],\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "print(pivot_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Another analysis we performed is to observe the trend of distributions of nutrition information based on `rating`. Recipes **with meat** tend to have **higher** calories, protein, total fat, and saturated fat, while non-meat recipes show higher carbohydrate` and sugar values. By observing these trends, we can identify which features may have predictive/determining power for our classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table = cleaned_df.pivot_table(\n",
    "    index='rating',\n",
    "    values=['minutes', 'calories', 'total_fat', 'sugar', 'protein', 'saturated_fat', 'carbohydrates','n_ingredients'],\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "\n",
    "print(pivot_table.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the **nutritional** metrics (calories, carbohydrates, protein, saturated fat, sugar, and total fat) show a fairly clear negative trend with increasing rating—lower values tend to correspond to higher ratings (at least from ratings 1 to 4). The decrease in calories, carbs, and fats from ratings 1 to 4 might reflect a preference for recipes that are perceived as healthier or less indulgent. However, the slight reversal in rating 5 (longer minutes, higher `fat`/`sugar`) could suggest that the most highly rated recipes balance healthiness with complexity or flavor richness that requires more preparation time and slightly richer ingredients.\n",
    "\n",
    "Most metrics **decrease** as the rating increases, suggesting that higher ratings (in this range) might be associated with healthier or less energy-dense options. There are some small increase sin several values at rating 5 indicates that while there’s an overall downward trend, the highest rating category might not follow the pattern as closely as ratings 1 to 4, so there might be some outliers existing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Assessment of Missingness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three columns in the cleaned dataframe that have non-trivial number missing entries are `rating`(**15036**), `review`(**2777**), and `description`(**136**).\n",
    "\n",
    "### NMAR Analysis\n",
    "In these three columns, we think that **none** is **NMAR**. \n",
    "\n",
    "In fact, we believe that all three columns all could be **MAR** due to the fact that they tend to correlate with each other. For instance, people who like the recipe will likely to give `rating` and also leave some positive feedback and `review` or the `description` of the dish itself to express his or her enjoyment. Also, the activeness/number of the `review`, `description`, and the `rating` could be also dependent on the `contributor_id` and the `name` of the recipe. One user could be really proactive in delievering messages or leave comments. One recipe may be really special/seasonal/memroable or for is designed for a specific holiday, leading to more views and attention. \n",
    "\n",
    "Specifically, we believe that `rating` is strongly likely to be influneced/correlated with other columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missingness Dependency\n",
    "First, we would like to investigate whether the number of ingredients has an impact on the missingness of `rating`.  \n",
    "\n",
    "***`n_ingredients` and `rating`'s missing dependecy***\n",
    "\n",
    "**Null Hypothesis:** The missingness of ratings does not depend on the number of ingredients(`n_ingredients`) in the recipe.\n",
    "\n",
    "**Alternative Hypothesis:** The missingness of ratings does depend on the number of ingredients(`n_ingredients`) in the recipe.\n",
    "\n",
    "**Test Statistic:** the absolute difference in mean `n_ingredients` between missing and non-missing rating groups.\n",
    "\n",
    "**Significance level:** 0.01\n",
    "\n",
    "Here is our **result**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from scipy.stats import norm\n",
    "\n",
    "df = cleaned_df.copy()\n",
    "df = df.reset_index()[['name', 'n_ingredients', 'rating']]\n",
    "\n",
    "# Create missingness indicator for rating\n",
    "df[\"rating_missing\"] = df[\"rating\"].isna()\n",
    "\n",
    "# Compute observed absolute difference in mean n_ingredients between missing and non-missing rating groups\n",
    "observed_diff_missing = abs(df.groupby(\"rating_missing\")[\"n_ingredients\"].mean().diff().iloc[-1])\n",
    "\n",
    "# Permutation test setup\n",
    "num_permutations = 1000\n",
    "perm_diffs_missing = []\n",
    "\n",
    "for _ in range(num_permutations):\n",
    "    permuted = df.copy()\n",
    "    permuted[\"rating_missing\"] = np.random.permutation(permuted[\"rating_missing\"])\n",
    "    perm_diff = abs(permuted.groupby(\"rating_missing\")[\"n_ingredients\"].mean().diff().iloc[-1])\n",
    "    perm_diffs_missing.append(perm_diff)\n",
    "\n",
    "# Compute p-value (two-sided)\n",
    "p_value_missing = np.mean(np.array(perm_diffs_missing) >= observed_diff_missing)\n",
    "\n",
    "# Plot permutation results\n",
    "fig_missing = px.histogram(perm_diffs_missing, nbins=30, \n",
    "                           title=\"Permutation Test: Missingness Dependency on n_ingredients\")\n",
    "fig_missing.add_vline(x=observed_diff_missing, line_dash=\"dash\", \n",
    "                      line_color=\"red\", annotation_text=\"Observed Abs Diff\")\n",
    "fig_missing.show()\n",
    "\n",
    "# Print results\n",
    "print(f\"Observed Absolute Difference in n_ingredients: {observed_diff_missing:.4f}\")\n",
    "print(f\"p-value: {p_value_missing:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "if p_value_missing < 0.01:\n",
    "    print(\"Reject H₀: Missingness of rating is dependent on n_ingredients\")\n",
    "else:\n",
    "    print(\"Fail to reject H₀: No strong evidence that missingness of rating depends on n_ingredients\")\n",
    "\n",
    "# Optionally, save the plot\n",
    "fig_missing.write_html('n_ingredients_missing.html', include_plotlyjs='cdn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our **Observed Absolute Difference in N_ingredients** is **0.1607**.\n",
    "We then get **p-value** of **0.0**, which is less than **0.01**.\n",
    "As a result, we **reject** the null hypothesis and conclude that the missingness of `rating` **does** depend on the number of ingredients(`n_ingredients`)\n",
    "\n",
    "Moving on, we would also like to see whether the duration of cooking has an impact on the missingness of `rating`.  \n",
    "\n",
    "***`minutes` and `rating`'s missing dependecy***\n",
    "\n",
    "**Null Hypothesis:** The missingness of ratings does not depend on the duration of cooking the recipe.\n",
    "\n",
    "**Alternative Hypothesis:** The missingness of ratings does depend on the duration of cookinghe recipe.\n",
    "\n",
    "**Test Statistic:** The absolute difference in mean `minutes` between missing and non-missing rating groups.\n",
    "\n",
    "**Significance level:** 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from scipy.stats import norm\n",
    "\n",
    "df = cleaned_df.copy()\n",
    "df = df.reset_index()[['name', 'minutes', 'rating']]\n",
    "\n",
    "# Create missingness indicator for rating\n",
    "df[\"rating_missing\"] = df[\"rating\"].isna()\n",
    "\n",
    "# Compute observed absolute difference in mean n_ingredients between missing and non-missing rating groups\n",
    "observed_diff_missing = abs(df.groupby(\"rating_missing\")[\"minutes\"].mean().diff().iloc[-1])\n",
    "\n",
    "# Permutation test setup\n",
    "num_permutations = 1000\n",
    "perm_diffs_missing = []\n",
    "\n",
    "for _ in range(num_permutations):\n",
    "    permuted = df.copy()\n",
    "    permuted[\"rating_missing\"] = np.random.permutation(permuted[\"rating_missing\"])\n",
    "    perm_diff = abs(permuted.groupby(\"rating_missing\")[\"minutes\"].mean().diff().iloc[-1])\n",
    "    perm_diffs_missing.append(perm_diff)\n",
    "\n",
    "# Compute p-value (two-sided)\n",
    "p_value_missing = np.mean(np.array(perm_diffs_missing) >= observed_diff_missing)\n",
    "\n",
    "# Plot permutation results\n",
    "fig_missing = px.histogram(perm_diffs_missing, nbins=30, \n",
    "                           title=\"Permutation Test: Missingness Dependency on minutes\")\n",
    "fig_missing.add_vline(x=observed_diff_missing, line_dash=\"dash\", \n",
    "                      line_color=\"red\", annotation_text=\"Observed Abs Diff\")\n",
    "fig_missing.show()\n",
    "\n",
    "# Print results\n",
    "print(f\"Observed Absolute Difference in minutes: {observed_diff_missing:.4f}\")\n",
    "print(f\"p-value: {p_value_missing:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "if p_value_missing < 0.01:\n",
    "    print(\"Reject H₀: Missingness of rating is dependent on minutes\")\n",
    "else:\n",
    "    print(\"Fail to reject H₀: No strong evidence that missingness of rating depends on minutes\")\n",
    "\n",
    "# Optionally, save the plot\n",
    "fig_missing.write_html('minutes_missing.html', include_plotlyjs='cdn')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our **Observed Absolute Difference in Minutes** is **51.4524**.\n",
    "We then get **p-value** of **0.1050**, which is greater than **0.01**.\n",
    "As a result, we **reject** the null hypothesis and conclude that the missingness of `rating` **does not** depend on the cooking time of the recipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the beginning, we would like to predict the rating of a recipe and we suspect that having `meat` in the recipes' `tags` have lower `rating` on average thnn those do not have.\n",
    "\n",
    "To investigate this question, we designed a **permutaiton test** where we shuffled the boolean column we created `contains_meat`. Our hypothesis, decision rule, statistics, prodcedures are as follows:\n",
    "\n",
    "**Null Hypothesis:** People rate meat and non-meat tagged dishes the same <br>\n",
    "\n",
    "**Alternative Hypothesis:** People rate recipes with **meat** tagged  lower than they rate non-meat tagged ones <br>\n",
    "\n",
    "**Decision Rule:** We will reject the null hypothesis if our p-value is less than **significance level** 0.05 <br>\n",
    "\n",
    "**Test Statistic:** We used **difference in mean** between ratings of non-meat dishes and meat dishes(with meat-without meat) as test stats since it is a directional test <br>\n",
    "\n",
    "**Prodcedures:**\n",
    "We run a 10000-simulation permutaiton test in order to get the empirical distribution of the test statistics under the null hypothesis. Since we lack prior information about the underlying population, so instead of relying on assumptions about the data, the permutation test allows us to directly assess whether the two observed distributions (recipes with meat and without meat) are likely to have come from the same population. We randomly shuffled the bool values given by `contains_meat`column many times and recalculating the rating differences each time, then we build an empirical distribution of the difference under the null hypothesis (that meat inclusion does not affect ratings).\n",
    "\n",
    "Here is our **result**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "df = cleaned_df.copy()\n",
    "df = df.reset_index()[['name', 'contains_meat', 'rating']]\n",
    "\n",
    "\n",
    "df_clean = df.dropna(subset=[\"rating\"])\n",
    "\n",
    "# Ensure that 'contains_meat' is boolean\n",
    "df_clean[\"contains_meat\"] = df_clean[\"contains_meat\"].astype(bool)\n",
    "\n",
    "\n",
    "observed_diff_rating = (\n",
    "    df_clean.groupby(\"contains_meat\")[\"rating\"].mean().loc[True]\n",
    "    - df_clean.groupby(\"contains_meat\")[\"rating\"].mean().loc[False]\n",
    ")\n",
    "\n",
    "num_permutations = 10000\n",
    "perm_diffs_rating = []\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(num_permutations):\n",
    "    permuted = df_clean.copy()\n",
    "    # Permute the contains_meat labels\n",
    "    permuted[\"contains_meat\"] = np.random.permutation(permuted[\"contains_meat\"])\n",
    "    perm_mean_diff = (\n",
    "        permuted.groupby(\"contains_meat\")[\"rating\"].mean().loc[True]\n",
    "        - permuted.groupby(\"contains_meat\")[\"rating\"].mean().loc[False]\n",
    "    )\n",
    "    perm_diffs_rating.append(perm_mean_diff)\n",
    "\n",
    "\n",
    "p_value_rating = np.mean(np.array(perm_diffs_rating) <= observed_diff_rating)\n",
    "\n",
    "\n",
    "fig_rating = px.histogram(\n",
    "    perm_diffs_rating,\n",
    "    nbins=30,\n",
    "    title=\"Permutation Test: Do Recipes with Meat Lead to Higher Ratings?\",\n",
    "    labels={\"value\": \"Mean Rating Difference (With Meat - Without Meat)\"}\n",
    ")\n",
    "fig_rating.add_vline(\n",
    "    x=observed_diff_rating,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"red\",\n",
    "    annotation_text=\"Observed Difference\",\n",
    ")\n",
    "fig_rating.show()\n",
    "\n",
    "# Print results\n",
    "print(f\"Observed Difference in Rating (With Meat - Without Meat): {observed_diff_rating:.4f}\")\n",
    "print(f\"p-value: {p_value_rating:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "if p_value_rating < 0.05:\n",
    "    print(\"Reject H₀: There is evidence that recipes with meat lead to lower ratings.\")\n",
    "else:\n",
    "    print(\"Fail to reject H₀: No strong evidence that meat inclusion affects ratings.\")\n",
    "\n",
    "\n",
    "fig_rating.write_html('permutation.html', include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our **Observed Difference in Rating** (With Meat - Without Meat) is **-0.0188**.\n",
    "We then get **p-value** of **0.0**, which is less than **0.05**.\n",
    "As a result, we **reject** the null hypothesis and conclude that people rate recipes with meat in their tag lower than recipes without meat in their tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Framing a Prediction Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will be **Predicting the Rating of a Recipe**, which is a **multi-class classification problem**, as `rating` consists only the integer from 1-5 (note we replace the **0** with **np.nan** at the beginning), which would be an catagorical, ordinal variable.\n",
    "\n",
    "We chose `rating` as our response variable as it is the statistic that best represent user satisfaction and overall quality. Additionally, we conduct several visualizations and analysis with `rating` as one of the variables and observed several trends across the distributions of `rating`. For instance, our hypothesis shows people rate recipes with meat in their tag **lower** than recipes without meat in their tag. So we may be able to predict the rating through features like inclusion of meat.\n",
    "\n",
    "At the time of prediction, some avaliable features include `contains_meat` and `minutes`, since they were all avalible as soon as the recipes were created before the actual ratings were given, we can safely use them as suitable predictors for our model.\n",
    "\n",
    "To evaluate our model's effectiveness, we will take a look at both the model's **accuracy** and **F1-score**. \n",
    "\n",
    "**Accuracy:** The accuracy will allow us to analyze overall how our model is doing in terms of creating predictions based on **given** parameters, in this case being `minute` and `contains_meat` <br>\n",
    "\n",
    "**F1-Score:**  The f1 socre will calculate harmonic mean of precision and recall for each class, weighted by the class frequency. As shown in the above analysis, we observed the imbalanced distribution for `rating`, which are heavily skewed left with most around 4 to 5. So, using such metric calculation ensures that the performance on less frequent rating categories is properly accounted for <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the **Random Forest Classifier** with two parameters(one **quantitative** one **nominal**) for the baseline model.\n",
    "\n",
    "1. `contains_meat`: As in the hypothesis section mentioned, people are likely to rate dishes with meat lower than the dishes without meat. This is a **nominal** variable contains **bool** value. We transformed the booleans into integers using **OneHotEncoder()** with 1 means True and 0 means False.\n",
    "\n",
    "2. `protein `: The `protein` column contains continuous, numerical values with **float** type, making it **quantitative**. Speicifcally, it represents the **percentage of daily value** of **protein** for the speicifc recipe. We used **StandardScaler()** to standardize the data.\n",
    "\n",
    "Additionally, we seperate the data into training group and testing group. The test group contains 20% of the data while the train group contians 80% of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=cleaned_df.dropna(subset='rating')\n",
    "\n",
    "X= df[['protein','contains_meat']]\n",
    "y=df['rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "\n",
    "preproc = make_column_transformer(\n",
    "    (StandardScaler(),['protein']),\n",
    "    (OneHotEncoder(drop = 'first'), ['contains_meat']),\n",
    "    remainder='passthrough',\n",
    ")\n",
    "pl = make_pipeline(preproc, RandomForestClassifier(  # Uses 100 separate decision trees\n",
    "    random_state=42))\n",
    "\n",
    "pl.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pl.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('Accuracy is:', accuracy)\n",
    "print(\"f1 score is:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the RandomForestClassifier from the pipeline.\n",
    "rf = pl.named_steps['randomforestclassifier']\n",
    "\n",
    "# Get the raw feature importances from the classifier.\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Get the column transformer from the pipeline.\n",
    "col_transformer = pl.named_steps['columntransformer']\n",
    "\n",
    "# If your transformers support get_feature_names_out, you can do:\n",
    "feature_names = col_transformer.get_feature_names_out()\n",
    "\n",
    "# Now, you can pair each feature name with its importance:\n",
    "for name, importance in zip(feature_names, importances):\n",
    "    print(f\"{name}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics produced by the baseline model is as follows:\n",
    "\n",
    "**Accuracy** 0.7736046856127077\n",
    "\n",
    "**F1-Score** 0.17489827929656016\n",
    "\n",
    "\n",
    "Our baseline has a pretty high accuracy but low F1-score. The f1-score shows that our baseline model is not overconfident and does not miss to many actual positives. In other words while the model is often correct, it likely performs very poorly on one or more classes—usually the minority class—resulting in low precision, recall, or both when these are combined as the F1 score. This is most likely due to the **imbalanced** distribution, as mentioned above during our analysis in EDA.\n",
    "\n",
    "To improve our model, we decided to take a look into the feature importance:\n",
    "\n",
    "For two of our **raw features: protein, contains_meat_True**, we have correponding transformed feature importance:\n",
    "\n",
    "**standardscaler__protein** 0.9789\n",
    "\n",
    "**onehotencoder__contains_meat_True** 0.0211\n",
    "This shows that the model relies heavily on the protein feature to make predictions, while the indicator for whether a recipe contains meat plays a much smaller role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build a final model, we decided to do another round of exploratory analysis of all the **numerical** columns. Speicifcally, we would like to examine and take a look at the **distribution** and **correlation** with target column `rating` of each.\n",
    "\n",
    "First, we started off with create a temporary dataframe **df** for simplicity. It is a copy of our **cleaned_df** with all rows containing at least one **np.nan** being removed.\n",
    "\n",
    "To make full use of our **datetime** columns, we split the **submitted** columns based on `day`, `month`, and `year`, and created a column for each, and we convert `date` columns to **ordinal** numbers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = cleaned_df.copy().reset_index().dropna()\n",
    "df['day'] = df['submitted'].dt.day\n",
    "df['month'] = df['submitted'].dt.month\n",
    "df['year'] =df['submitted'].dt.year\n",
    "df['date_ordinal'] = df['date'].apply(lambda x: x.toordinal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then created another dataframe **df_numeric**, extracting all the relevant **numerical** columns from **df** we would like to examine and stored the names in a varibel **relevant_columns**, containing `minutes`,`calories`, `total_fat`, `sugar`, `sodium`, `protein`, `saturated_fat`, `carbohydrates`, `n_steps`, `n_ingredients`, `submitted`, `date_ordinal`, `day`,`year`,`month`.\n",
    "\n",
    "Looping through each column, we create graphs to visualize each **distribution**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df.select_dtypes(include=[np.number])\n",
    "relevant_columns = [['minutes', 'n_steps', 'n_ingredients','calories',\n",
    "       'total_fat', 'sugar', 'sodium', 'protein', 'saturated_fat',\n",
    "       'carbohydrates', 'day', 'month', 'year','date_ordinal']]\n",
    "df_numeric = df_numeric[relevant_columns]\n",
    "for col in relevant_columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made some key observations after the visualizations:\n",
    "\n",
    "1. Some distributions like `n_steps` and `n_ingredients` look fairly symmetric, indicating that the mean and median are likely similar. For these we can just apply some simple **StandardScaler()**.\n",
    "\n",
    "2. Other distributions especially for those relevant to **nutrition**, such as `protein` and `saturated_fat`, and `minutes` show clear skewness (either right‐skewed or left‐skewed), which implies that a few extreme values are pulling the mean away from the median. For these columns, we might need to apply potential transoformers and scalers like **log or exponential transformation** or **RobustScaler()**.\n",
    "\n",
    "3. Most of the **datetime** type columns like `day` and `month` appeared to be relatively even, uniform, and symmetric.\n",
    "\n",
    "In general, our data seems to be quite **imbalanced**. Due to this consideration, for **model selection**, we decided to keep using **Random Forest Classifiers**, which is tend to be robust to outliers and skewed distributions because they split data based on thresholds and are less affected by extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we decided to take more insights by looking into the **linear correlation** of each with our targeted/predicted column `rating`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_scores = df_numeric.corr()['rating'].sort_values(ascending=False)\n",
    "correlation_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many variables and **nutritional metrics** (total_fat, saturated_fat, sodium, sugar, calories, protein, carbohydrates) have correlations near zero (ranging from about **0.02463** to **-0.012863**), indicates that these factors have **little** linear association with the rating in this dataset.\n",
    "\n",
    "This further proves that in order to increase predictive power, we have to create more informative features by using techniques such as **creating interaction terms** and **apply transformations** that better capture the underlying relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Numerical(Nutritional) Features:\n",
    "\n",
    "`calories`, `protein`, `total fat`, `sugar`, `sodium`, `saturated fat`, and `carbohydrates`\n",
    "\n",
    "According to the pivot table, we know that nutritional information generally are good indicator for `rating`.(As `rating` increases, in general, all nutritional features decrease)\n",
    "To address skewness and redundancy as shown in the above analysis, a custom log transformation (np.log1p) is applied to nutrition features, and then we utilized **RobustScaler()** to scale them considering the high potential of outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Define log-transform for nutrition features\n",
    "# ===============================\n",
    "def log_transform_nutrition(X):\n",
    "    X = X.copy()\n",
    "    nutrition_cols = ['calories', 'protein', 'total_fat', 'sugar', 'sodium', 'saturated_fat', 'carbohydrates']\n",
    "    for col in nutrition_cols:\n",
    "        # Use np.log1p to handle zeros gracefully\n",
    "        X[col] = np.log1p(X[col])\n",
    "    return X\n",
    "\n",
    "# ===============================\n",
    "# Nutrition pipeline: log-transform, scale, then apply PCA\n",
    "# ===============================\n",
    "nutrition_pipeline = SkPipeline([\n",
    "    ('log_transform', FunctionTransformer(log_transform_nutrition, validate=False)),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Preparation Details:\n",
    "\n",
    "`minutes`,`n_ingredients` and  `n_steps`\n",
    "\n",
    "Features that are normally distributed like number of steps, and ingredients are scaled using **StandardScaler()**. We also know from previous analysis that higher rated recipes are more time-consuming and require more comlex procedures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Pipeline for other numeric features (e.g., minutes, n_steps, n_ingredients)\n",
    "# ===============================\n",
    "other_numeric_columns = ['minutes', 'n_steps', 'n_ingredients']\n",
    "other_numeric_pipeline = SkPipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Categorical Features:\n",
    "\n",
    "`contains_meat`\n",
    "\n",
    "We keep using the **OneHotEncode()** as shown in our baseline model as it helps with achieving decent accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Pipeline for categorical feature: contains_meat\n",
    "# ===============================\n",
    "categorical_pipeline = SkPipeline([\n",
    "    ('onehot', OneHotEncoder(drop='first'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Textual Features:\n",
    "\n",
    "`tags`\n",
    "\n",
    "Since it contians a list of tags for a given recipe, we preprocessed them by cleaning, lowercasing, and concatenating individual tags. They are then transformed into numerical features using TF-IDF. We think that there might be certain tags that are associated with higher views, search, and attentions, such as those related to holidays or special diet plan.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ===============================\n",
    "# TF-IDF Transformer for Tags\n",
    "# ===============================\n",
    "def join_tags(x):\n",
    "    if isinstance(x, list):\n",
    "        cleaned = [str(item).replace(\"'\", \"\").replace('\"', \"\").strip().lower() for item in x]\n",
    "        joined = \" \".join(cleaned)\n",
    "    else:\n",
    "        joined = str(x).strip().lower()\n",
    "    return joined if joined else \"none\"\n",
    "\n",
    "class TfidfTagsTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_df=1, stop_words=None, **kwargs):\n",
    "        self.min_df = min_df\n",
    "        self.stop_words = stop_words\n",
    "        self.kwargs = kwargs\n",
    "        self.vectorizer = TfidfVectorizer(min_df=self.min_df,\n",
    "                                          stop_words=self.stop_words,\n",
    "                                          **self.kwargs)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.squeeze()\n",
    "        X_text = [join_tags(x) for x in X]\n",
    "        self.vectorizer.fit(X_text)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.squeeze()\n",
    "        X_text = [join_tags(x) for x in X]\n",
    "        return self.vectorizer.transform(X_text)\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.vectorizer.get_feature_names_out()\n",
    "\n",
    "tags_pipeline = SkPipeline([\n",
    "    ('tfidf_tags', TfidfTagsTransformer(min_df=1, stop_words='english'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Date Features:\n",
    "\n",
    "`submitted` and `date`\n",
    "\n",
    "Since these were columns containing datetime objects, we think that recipes rating might vary in certain holidays, seasons, culture, societal trend, and other timeframe. Also, we found that higher ratings are given in earlier submitted years. To account for these, we further extract day, month, year, day of week, and ordinal values from the submitted and recipe dates to capture temporal patterns. We then applied **StandardScaler()** to scale them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ===============================\n",
    "# Date Pipeline: extract date features and scale them\n",
    "# ===============================\n",
    "def extract_date_features(X):\n",
    "    X = X.copy()\n",
    "    # Parse dates (assuming valid formats since there are no missing values)\n",
    "    X['submitted'] = pd.to_datetime(X['submitted'], errors='coerce')\n",
    "    X['date'] = pd.to_datetime(X['date'], errors='coerce')\n",
    "    \n",
    "    # Extract basic date features\n",
    "    X['day'] = X['submitted'].dt.day\n",
    "    X['month'] = X['submitted'].dt.month\n",
    "    X['year'] = X['submitted'].dt.year\n",
    "    X['day_of_week']=X['submitted'].dt.dayofweek\n",
    "\n",
    "    # Convert dates to ordinal values\n",
    "    X['submitted_ordinal'] = X['submitted'].apply(lambda x: x.toordinal())\n",
    "    X['date_ordinal'] = X['date'].apply(lambda x: x.toordinal())\n",
    "    \n",
    "    return X[['day', 'month', 'year', 'day_of_week','submitted_ordinal', 'date_ordinal']]\n",
    "\n",
    "date_pipeline = Pipeline([\n",
    "    ('date_features', FunctionTransformer(extract_date_features, validate=False)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Asemble All the pipelines into final pipeline with Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# Combine all pipelines using ColumnTransformer\n",
    "# ===============================\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('nutrition', nutrition_pipeline, ['calories', 'protein', 'total_fat', 'sugar', 'sodium', 'saturated_fat', 'carbohydrates']),\n",
    "    ('other_num', other_numeric_pipeline, other_numeric_columns),\n",
    "    ('cat', categorical_pipeline, ['contains_meat']),\n",
    "    ('tags', tags_pipeline, ['tags']),\n",
    "    ('dates', date_pipeline, ['submitted', 'date'])\n",
    "], remainder='drop')\n",
    "\n",
    "# ===============================\n",
    "# Final Pipeline with Classifier\n",
    "# ===============================\n",
    "final_pipeline = SkPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('clf', RandomForestClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Split data and Set up grid search, train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = df[['calories', 'protein', 'total_fat', 'sugar', 'date', 'tags', 'sodium', 'saturated_fat', 'submitted',\n",
    "        'carbohydrates', 'minutes', 'n_steps', 'n_ingredients', 'contains_meat']]\n",
    "y = df['rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "param_grid = {\n",
    "    'clf__max_depth': [3, 5, 7, None],\n",
    "    'clf__n_estimators': [100, 200, 300, 500],\n",
    "    'clf__min_samples_split': [2, 5, 10],\n",
    "    'clf__min_samples_leaf': [1, 2, 4],\n",
    "    'clf__max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(final_pipeline, param_grid, cv=kf, scoring='f1_macro', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validated f1_macro:\", grid_search.best_score_)\n",
    "\n",
    "final_model = grid_search.best_estimator_\n",
    "test_accuracy = final_model.score(X_test, y_test)\n",
    "print(\"Test Set Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Fairness Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our fairness analysis, we decided to use the column `contains_meat` to create the group.\n",
    "**Group X :** Defined as all recipes where the boolean column `contains_meat` is True.\n",
    "**Group Y :** Defined as all recipes where the boolean column `contains_meat` is False.\n",
    "We used **precision/recall parity** as our evaluation metric, as seen before. The metric calculates the precision for each class and then takes the average, ensuring that performance across all classes is considered equally. In imbalanced datasets or multi-class settings, overall accuracy can be misleading. It’s more important to focus on **False Positives**, that is, for the model to correctly identify the rating of a recipe among all instances of that rating. False positives would not be good since it would mislead users with the incorrectly labeled ratings.\n",
    "\n",
    "**Null Hypothesis:** The model is fair. That is, the difference in macro precision between recipes with meat (Group X) and recipes without meat (Group Y) is zero, and any observed difference is due to random chance. <br>\n",
    "\n",
    "**Alternative Hypothesis:** The model is unfair. In other words, there is a systematic difference in macro precision between the two groups (specifically, that recipes without meat have a higher precision than those with meat). <br>\n",
    "\n",
    "**Decision Rule:** We will reject the null hypothesis if our p-value is less than **significance level** 0.05\n",
    "\n",
    "**Test Statistic:** We used **difference in macro precision** between between the two groups(without meat-with meat) as our test statistic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Get predictions from the final model on the test set.\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "group_labels = X_test['contains_meat'].values\n",
    "\n",
    "\n",
    "mask_meat = group_labels == True    # Group X: recipes with meat\n",
    "mask_no_meat = group_labels == False  # Group Y: recipes without meat\n",
    "\n",
    "precision_meat = precision_score(y_test[mask_meat], y_pred[mask_meat], average='macro', zero_division=0)\n",
    "precision_no_meat = precision_score(y_test[mask_no_meat], y_pred[mask_no_meat], average='macro', zero_division=0)\n",
    "\n",
    "\n",
    "observed_diff = precision_no_meat - precision_meat\n",
    "\n",
    "print(\"Observed Precision:\")\n",
    "print(\"Recipes WITHOUT meat (contains_meat == False):\", precision_no_meat)\n",
    "print(\"Recipes WITH meat (contains_meat == True):\", precision_meat)\n",
    "print(\"Observed Difference (non-meat minus meat):\", observed_diff)\n",
    "\n",
    "num_permutations = 1000\n",
    "permuted_diffs = np.zeros(num_permutations)\n",
    "\n",
    "for i in range(num_permutations):\n",
    "\n",
    "    permuted_labels = np.random.permutation(group_labels)\n",
    "    \n",
    "    perm_mask_meat = permuted_labels == True\n",
    "    perm_mask_no_meat = permuted_labels == False\n",
    "    \n",
    "    perm_precision_meat = precision_score(y_test[perm_mask_meat], y_pred[perm_mask_meat], average='macro', zero_division=0)\n",
    "    perm_precision_no_meat = precision_score(y_test[perm_mask_no_meat], y_pred[perm_mask_no_meat], average='macro', zero_division=0)\n",
    "    \n",
    "    permuted_diffs[i] = perm_precision_no_meat - perm_precision_meat\n",
    "\n",
    "\n",
    "p_value = np.mean(permuted_diffs >= observed_diff)\n",
    "print(\"Permutation Test p-value:\", p_value)\n",
    "\n",
    "alpha = 0.01  # significance level\n",
    "if p_value < alpha:\n",
    "    print(f\"At the significance level of {alpha}, we reject the null hypothesis. The model appears to be unfair.\")\n",
    "else:\n",
    "    print(f\"At the significance level of {alpha}, we fail to reject the null hypothesis. The observed difference may be due to chance.\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(permuted_diffs, bins=30, alpha=0.75, color='skyblue', edgecolor='black')\n",
    "plt.axvline(observed_diff, color='red', linestyle='dashed', linewidth=2,\n",
    "            label=f'Observed Diff = {observed_diff:.4f}')\n",
    "plt.xlabel(\"Difference in Macro Precision (Non-meat minus Meat)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Permutation Test: Precision Difference Distribution\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our **Observed Difference in scores** is **0.13079558064360503**.\n",
    "We then get **p-value** of **0.071**, which is less than **0.01**.\n",
    "As a result, we **fail to reject** the null hypothesis. This suggests that the observed difference in precision could reasonably be due to random chance, and we conclude that there is no statistically significant evidence proving that our model is unfair.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
